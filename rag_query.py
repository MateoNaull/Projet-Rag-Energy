# rag_query.py
from mistralai import Mistral
from dotenv import load_dotenv
import os
import chromadb
from sentence_transformers import SentenceTransformer

# ===== 1) Charger variables d'environnement =====
load_dotenv()
api_key = os.getenv("MISTRAL_API_KEY")
if not api_key:
    raise ValueError("âŒ Pas de clÃ© API MISTRAL trouvÃ©e. VÃ©rifie ton .env")

# ===== 2) Initialiser client Mistral =====
llm_client = Mistral(api_key=api_key)

# ===== 3) Charger lâ€™index Chroma =====
client = chromadb.PersistentClient(path="outputs/index")
collection = client.get_collection("energie_rag")

# Embeddings doivent Ãªtre les mÃªmes que dans ingest.py
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# ===== 4) Fonction RAG =====
def rag_query(question: str, k: int = 3):
    # Encoder la question
    q_embedding = embedder.encode(question).tolist()

    # RÃ©cupÃ©rer les chunks les plus proches
    results = collection.query(query_embeddings=[q_embedding], n_results=k)

    documents = results["documents"][0]
    sources = results["metadatas"][0]

    # Contexte combinÃ©
    context = "\n\n".join(documents)

    # Prompt pour Mistral
    prompt = f"""
    Tu es un assistant expert en Ã©nergie.
    Utilise les passages suivants pour rÃ©pondre Ã  la question de maniÃ¨re concise et claire.

    Contexte :
    {context}

    Question :
    {question}

    RÃ©ponse :
    """

    # Appel au LLM
    resp = llm_client.chat.complete(
        model="mistral-small-latest",
        messages=[{"role": "user", "content": prompt}]
    )

    answer = resp.choices[0].message.content
    return answer, documents, sources

# ===== 5) Exemple =====
if __name__ == "__main__":
    question = "Comment est rÃ©parti le bouquet Ã©nergÃ©tique franÃ§ais ?"
    answer, docs, sources = rag_query(question)

    print("ğŸ” Question :", question)
    print("\nğŸ“š Passages retenus :")
    for d in docs:
        print("-", d[:200], "...")
    print("\nğŸ“Œ RÃ©ponse gÃ©nÃ©rÃ©e :\n", answer)
