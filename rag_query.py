# src/rag_query.py

import chromadb
from transformers import pipeline

# ========== 1) Connexion √† Chroma ==========
client_chroma = chromadb.PersistentClient(path="outputs/index")
collection = client_chroma.get_or_create_collection("energie_rag")

# ========== 2) Charger un mod√®le Hugging Face ==========
# Flan-T5-base : petit mod√®le d'instruction, tourne sur CPU
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)

# ========== 3) Recherche dans l'index ==========
def query_index(question: str, top_k: int = 3):
    results = collection.query(
        query_texts=[question],
        n_results=top_k
    )
    return results

# ========== 4) G√©n√©ration de r√©ponse ==========
def summarize_passage(text, max_tokens=100):
    result = generator(
        f"R√©sume en quelques phrases le texte suivant : {text}",
        max_new_tokens=max_tokens
    )
    return result[0]["generated_text"]

def generate_answer(question, docs):
    # R√©sumer chaque passage
    summaries = [summarize_passage(doc) for doc in docs]
    context = "\n\n".join(summaries)

    prompt = f"""
    Tu es un assistant sp√©cialis√© en √©nergie.
    Question : {question}

    Contexte (r√©sum√©s des documents) :
    {context}

    R√©ponds uniquement √† partir du contexte fourni, de fa√ßon claire et concise.
    """
    result = generator(prompt, max_new_tokens=256)
    return result[0]["generated_text"]


# ========== 5) Exemple d‚Äôutilisation ==========
if __name__ == "__main__":
    question = "Quelle est la consommation √©nerg√©tique en √éle-de-France ?"
    results = query_index(question, top_k=3)

    docs = results["documents"][0]

    print("\nüîé Question :", question)

    print("\nüìå R√©ponse g√©n√©r√©e :\n")
    answer = generate_answer(question, docs)
    print(answer)
